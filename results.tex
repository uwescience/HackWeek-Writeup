\section*{Results}

\begin{figure*}[h!]
%\begin{center}
%\begin{subfigure}[t]{\textwidth}
\centering
%\caption{}
\includegraphics[width=\textwidth]{f2.eps}
%\end{subfigure}
\caption{Post-workshop surveys from three hack weeks: participants in the 2016 astro-, geo- and neuro- hack weeks responded to questions assessing their experiences. Response rates are reported in the panel titles for AHW, GHW and NHW, respectively. We report here about results in three different domains: the development of technical skills (a -- c), collaboration and teaching (d -- f), and shifts in attitudes towards reproducibility and open science (g, h).}
\label{fig:survey}
%\end{center}
\end{figure*}

Measuring the success of a hack week objectively is complicated by the variety of goals that a hack week might have (see above).
Additionally, the participant-driven format facilitates knowledge transfer and collaborations in sometimes surprising ways that escape traditional measures of success.

One key metric is the number of publications that result from hack week projects, but this is a fairly narrow definition of success, in line with standard academic performance indicators.
Assuming that participants work largely in the open during a hack week, and that most projects have a strong programming component, another indicator of success is the activity of participants in terms of code written and committed to a public code repository.
Still, these measures ignore learning, community-building as well as networking outcomes, which can be assessed through post-workshop surveys.
Here, we have taken an approach that combines these metrics: we start with survey results, and anecdotally report about publications and projects generated (see the following section).

Focusing on the outcomes of astro-, geo- and neuro- hack weeks (AHW, GHW, NHW, respectively) from 2016 and 2017, we find that most participants self-reported successful learning outcomes (AHW: 76\%, GHW: 89\%, NHW: 79\% for responses ``somewhat agree'', ``agree'' and ``strongly agree''; Figure \ref{fig:survey}, (a)).
The overwhelming majority of respondents at the hack weeks ($>95\%$ for all three events) believed that they learned things that improve their day-to-day research, and that attendance has made them a better scientist  (Figure \ref{fig:survey}, (b, c)).
%More specifically, we compared learning outcomes in data visualization (the only topic explicitly shared between all three hack weeks; Figure \ref{fig:survey}, (d, e). We asked participants to subjectively rate their knowledge before the hack week, and find skill levels to be broadly distributed. This is expected, given the goal of diversity during the selection stage. We find that most participants have positive learning outcomes at all three hack weeks for data visualization, but that outcomes vary strongly, as is expected, too, with a group that is this diverse. We also note that there might be systematic effects and biases due to the subjectivity of rating knowledge and learning outcomes.
Because peer learning is a major mode of knowledge transfer at hack weeks, we asked participants whether they taught other participants.
We find that again a majority agreed with this statement to some degree (AHW: 79\%, GHW: 69\%, NHW: 75\%; Figure \ref{fig:survey}, (d)), though responses are not as unequivocal as they are in some of the other categories.
The majority of participants felt that they built valuable connections to other researchers (Figure \ref{fig:survey}, (e)), especially at NHW, where more than 64\% of participants strongly agreed with this statement.

Given the diversity in skills and backgrounds of participants admitted to our hack weeks, a dependence of learning outcomes and teaching on career stage is plausible. At the same time, we suggested earlier in this paper that peer learning is a major mode of knowledge transfer for participants of all career stages. We find no strong evidence for a significant difference between early-career researchers and senior participants for any of the three hack weeks for questions regarding learning outcomes of new tools and topics (Figure \ref{fig:survey}, panel (a)), improvements in day-to-day research (Figure \ref{fig:survey}, (b)) or overall improvements in science (Figure \ref{fig:survey}, (c)) ($p > 0.0007$ (trial-corrected significance threshold) for all with zero or small effect size; data and all (including non-significant) correlations are presented in SM, Section 2, Table 1 and SM Section 3, Figures 1-12). Only for GHW do we find a very weak suggestion that early-career researchers might agree more strongly that the hack week improved their day-to-day research (p=0.02) with a large effect size of $\phi_c = 0.35$ for $1.88$ degrees of freedom (following~\citep{cohen1988}).
Similarly, we find no indication ($p > 0.0007$) that late-career researchers self-report a higher level of teaching at hack weeks compared to early-career researchers (Figure \ref{fig:survey}, (d)).

One important question is whether participants from underrepresented groups thrive at hack weeks, or whether their full participation is impeded. Significant differences between minorities and non-minorities, even on a self-reported scale, for questions related to learning outcomes, teaching or network building, would indicate that improvements in workshop facilitation and structure may be required to allow a wide range of participants to participate fully.
Based on our surveys, we find no significant dependence of self-reported learning outcomes on gender identity or race/ethnicity ($p > 0.0007$). Similarly, minority participants with respect to gender identity and race or ethnicity do not significantly differ in their answers with respect to teaching outcomes for any of the hack weeks ($p > 0.0007$). Attitudes towards building valuable connections at all three hack weeks or the value of their contributions to their hack teams also do not differ with respect to gender or racial/ethnic identity. For GHW, there is a weak indication that participants from racial/ethnic minorities responded more positively when asked about building connections ($p=0.04$) with a medium to large effect size ($\phi_c = 0.4$; $\mathrm{dof}_{\phi_c} = 0.97$), while for AHW, responses regarding the value of contributions to hack teams may be spread more widely for participants from racial/ethnic minorities than for Caucasian participants ($p = 0.01$; $\phi_c = 0.4$, $\mathrm{dof}_{\phi_c} = 0.98$). These initial results indicate that future work on how demographics interact with hack week attendance may be fruitful.

We find that the hack weeks have been largely successful in promoting positive attitudes towards reproducibility and open science: at all three events, the majority reported that the hack week has made them more comfortable with open science  (GHW: 97\%; NHW: 95\%; AHW: 72\%, Figure \ref{fig:survey}, (h)), and more than 85\% of all participants (AHW: 86\%, GHW: 94\%, NHW: 95\%; Figure \ref{fig:survey}, (g)) put code or data created at the hack week into a public repository.
%The overall behaviour hints at how general conventions and attitudes differ in different fields.
%Astronomy shows the largest degree of openness toward open science, whereas our results indicate that open science is still fairly uncommon in the geosciences, with neuroscience falling in between.
%Similar attitudes are reflected when asking whether the hack week has made participants more comfortable with open science (Figure \ref{fig:survey}, (j)): again, geoscience shows the large improvement with over 97\% agreeing with this statement to some degree, followed by neuroscience (95\%) and astronomy (72\%).
%Overall, our results indicate that hack weeks are effective at addressing persisting doubts about making research open and reproducible.
While the focus on open science is not necessarily a required component of a hack week, it aligns naturally with many of the goals and values commonly promoted at hack weeks, such as production of open-source software and data sharing. In some fields, especially where ethical issues around data sharing and privacy are relevant, this might not be a desired focus of the hack week, or might be replaced or augmented by a discussion of ethical considerations.

We caution the reader that our survey results should be read only as an initial indication of the hypotheses we proposed about the use and outcomes of hack weeks, in line with the surveys' exploratory nature. The number of respondents is small and the effects likely subtle. This means that the lack of significant differences may be caused by the small statistical power of our sample. The most important independent variable--attendance at the hack week--is something that our current survey design cannot account for. Moreover, self-reported learning outcomes are not an objective measure, because they are likely subject to internal biases of respondents. Future work will include a more refined survey design as well as attendance versus non-attendance as a control variable.

Because all three events are relatively recent, it is still early to evaluate long-term outcomes, as well as others including publications and collaborations resulting from these events.
There are, however, initial indicators that all hack weeks encouraged long-term engagement with new concepts or tools and that they directly resulted in a number of publications \cite{gullysantiago2015,faria2016,keshavan2017,leonard2017,jordan2017,peterson2017,hahn2017,pricewhelan2017}. For specific examples, see also below.

\subsection*{Examples of Hack Week Outcomes}
\label{sec:outcomes}
\subsubsection*{Example 1: Astro Hack Week}
In 2015, a small team used AHW to found a new software project called Stingray\footnote{https://github.com/StingraySoftware/stingray} with the goal of providing implementations of time series analysis algorithms often used in astronomy.
Astro Hack Week enabled participants to seed a new collaboration around a software project needed by the larger community, facilitated by the collaborative environment at AHW.
Stingray has since matured into an enduring collaboration within the community with five active maintainers and four Google Summer of Code projects.
\subsubsection*{Example 2: Geo Hack Week}
In 2016, a GHW project team used Google Earth Engine to explore spatial patterns in climate, topography and population data with the goal of mapping the most suitable locations for renewable energy sites in the United States.
The team used machine learning algorithms in conjunction with the powerful hardware resources provided by Google Earth Engine\footnote{\url{http://georgerichardson.net/2017/04/10/searching-for-energy-in-a-random-forest/}}.
%George Richardson, one of the project leads, now works for a renewable resource company in Seattle.
\subsubsection*{Example 3: Neuro Hack Week}
During NHW 2016, one of the teams analyzed an openly available data-set of MRI data from children\cite{Di_Martino2014-qf}, to test the effects of motion on the results, using varying motion cut-offs.
%They tested both the split-half reliability of an analysis of brain connectivity, as well as an analysis that used machine learning to distinguish between brains of children with and without autism spectrum disorder.
The team (four researchers from  different institutions) continued to work on this project remotely after the end of NHW, and eventually published a paper describing these results \cite{leonard2017}.
