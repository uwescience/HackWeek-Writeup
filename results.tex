\section*{Results}

\begin{figure*}[h!]
%\begin{center}
%\begin{subfigure}[t]{\textwidth}
\centering
%\caption{}
\includegraphics[width=\textwidth]{f2.eps}
%\end{subfigure}
\caption{Post-workshop survey responses from the 2016 astro-, geo- and neuro- hack weeks. Response rates are in the panel titles. Results presented in three different domains: the development of technical skills (a -- c), collaboration and teaching (d -- f), and shifts in attitudes towards reproducibility and open science (g, h).}
\label{fig:survey}
%\end{center}
\end{figure*}

Measuring the success of a hack week objectively is complicated by the variety of goals that a hack week might have (see above).
Additionally, the participant-driven format facilitates knowledge transfer and collaborations in sometimes surprising ways that escape traditional measures of success.

One key metric is the number of publications that result from hack week projects, but this is a fairly narrow definition of success, in line with standard academic performance indicators.
Assuming that participants work largely in the open during a hack week, and that most projects have a strong programming component, another indicator of success is the activity of participants in terms of code written and committed to a public code repository.
Still, these measures ignore learning, community-building as well as networking outcomes, which can be assessed through post-workshop surveys.
Here, we have taken an approach that combines these metrics: we start with survey results, and anecdotally report about publications and projects generated (see the following section).

Focusing on the outcomes of astro-, geo- and neuro- hack weeks (AHW, GHW, NHW, respectively) from 2016 and 2017, we find that most participants self-reported successful learning outcomes (AHW: 76\%, GHW: 89\%, NHW: 79\% for responses ``somewhat agree'', ``agree'' and ``strongly agree''; Figure \ref{fig:survey}, (a)).
The overwhelming majority of respondents at the hack weeks ($>95\%$ for all events) believed that they learned things that improve their day-to-day research, and that attendance has made them a better scientist  (Figure \ref{fig:survey}, (b, c)).
%More specifically, we compared learning outcomes in data visualization (the only topic explicitly shared between all three hack weeks; Figure \ref{fig:survey}, (d, e). We asked participants to subjectively rate their knowledge before the hack week, and find skill levels to be broadly distributed. This is expected, given the goal of diversity during the selection stage. We find that most participants have positive learning outcomes at all three hack weeks for data visualization, but that outcomes vary strongly, as is expected, too, with a group that is this diverse. We also note that there might be systematic effects and biases due to the subjectivity of rating knowledge and learning outcomes.
Because peer learning is a major mode of knowledge transfer at hack weeks, we asked participants whether they taught other participants.
We find that again a majority agreed with this statement to some degree (AHW: 79\%, GHW: 69\%, NHW: 75\%; Figure \ref{fig:survey}, (d)), though responses are not as unequivocal as they are in some of the other categories.
The majority of participants felt that they built valuable connections to other researchers (Figure \ref{fig:survey}, (e)), especially at NHW, where more than 64\% of participants strongly agreed with this statement.

Given the diversity in skills and backgrounds of participants admitted to our hack weeks, a dependence of learning outcomes and teaching on career stage is plausible. At the same time, as suggested earlier, peer learning is a major mode of knowledge transfer for participants of all career stages. We find no strong evidence for a significant difference between early-career and senior participants in any of the hack weeks for questions regarding learning outcomes of new tools and topics (Figure \ref{fig:survey}, panel (a)), improvements in day-to-day research (Figure \ref{fig:survey}, (b)) or overall improvements in science (Figure \ref{fig:survey}, (c)) ($p > 0.0007$ (trial-corrected significance threshold) with zero or small effect size for all). Data and all (including non-significant) correlations are presented in SM, Section 2, Table 1 and SM Section 3, Figures 1-12. Only for GHW do we find that early-career researchers agree more strongly that the hack week improved their day-to-day research (p=0.02) with a large effect size of $\phi_c = 0.35$ for $1.88$ degrees of freedom (following~\citep{cohen1988}).
Similarly, we find no indication ($p > 0.0007$) that late-career researchers self-report a higher level of teaching at hack weeks compared to early-career researchers (Figure \ref{fig:survey}, (d)). However, the confidence intervals on the measured effect sizes are wide. Testing for the absence of an effect using an equivalence test on the effect size with an equivalence bound corresponding to a moderate effect size suggests that the data are currently not conclusive enough to reject a moderate to large effect ($ p_\mathrm{eq} > 0.0007$ for all four questions above).

One important question is whether participants from underrepresented groups thrive at hack weeks, or whether their full participation is impeded. Significant differences between minorities and non-minorities, even on a self-reported scale, for questions related to learning outcomes, teaching or network building, would indicate that improvements in workshop facilitation and structure may be required to allow members of these groups to participate fully.
In our surveys, we find no significant dependence of self-reported learning outcomes on gender identity or race/ethnicity ($p > 0.0007$). Similarly, for none of the hack weeks do minority participants differ significantly in their answers with respect to teaching outcomes, building valuable connections or the value of their contributions to their hack teams  ($p > 0.0007$). For GHW, there is an indication that participants from racial/ethnic minorities may respond more positively when asked about building connections ($p=0.04$) with a medium to large effect size ($\phi_c = 0.4$; $\mathrm{dof}_{\phi_c} = 0.97$), while for AHW, responses regarding the value of contributions to hack teams may be spread more widely for participants from racial/ethnic minorities than for Caucasian participants ($p = 0.01$; $\phi_c = 0.4$, $\mathrm{dof}_{\phi_c} = 0.98$). Equivalence tests reveal systematically small, though non-significant, $p$-values in the range of $p=0.02-0.05$ for AHW for all four questions above in conjunction with gender or ethnic/racial identity, while for GHW and NHW $p > 0.05$ for the same questions (note that the sample size for GHW and NHW was smaller by a factor of 2 compared to AHW). These results, while not a decisive exclusion of an effect of race/ethnicity or gender on hack week participation, provide an indication that our facilitation strategies may be effective in fostering participation. Future work on how demographics interact with hack week attendance may be fruitful.

We find that the hack weeks have been largely successful in promoting positive attitudes towards reproducibility and open science: at all three events, the majority reported that the hack week has made them more comfortable with open science  (GHW: 97\%; NHW: 95\%; AHW: 72\%, Figure \ref{fig:survey}, (h)), and more than 85\% of all participants (AHW: 86\%, GHW: 94\%, NHW: 95\%; Figure \ref{fig:survey}, (g)) put code or data created at the hack week into a public repository.
%The overall behaviour hints at how general conventions and attitudes differ in different fields.
%Astronomy shows the largest degree of openness toward open science, whereas our results indicate that open science is still fairly uncommon in the geosciences, with neuroscience falling in between.
%Similar attitudes are reflected when asking whether the hack week has made participants more comfortable with open science (Figure \ref{fig:survey}, (j)): again, geoscience shows the large improvement with over 97\% agreeing with this statement to some degree, followed by neuroscience (95\%) and astronomy (72\%).
%Overall, our results indicate that hack weeks are effective at addressing persisting doubts about making research open and reproducible.
While the focus on open science is not necessarily a required component of a hack week, it aligns naturally with many of the goals and values commonly promoted at hack weeks, such as production of open-source software and data sharing. In some fields, especially where ethical issues around data sharing and privacy are relevant, this should be augmented with a discussion of ethical considerations.

In line with the surveys' exploratory nature, these results should be read only as an initial indication of the hypotheses we proposed about the use and outcomes of hack weeks. The number of respondents is small and the effects likely subtle, and lack of significant differences may be due to statistical power in our sample. Furthermore, the most important independent variable -- attendance of a hack week -- is not accounted for in our current design. Moreover, self-reported learning outcomes are not an objective measure, because they are likely subject to response biases. Future work will include more refined survey design and inclusion of control non-attendees.

Because all three events are relatively recent, it is still early to evaluate long-term outcomes, including publications and collaborations resulting from these events.
There are, however, initial indicators that all hack weeks encouraged long-term engagement with new concepts or tools and that they directly resulted in a number of publications \cite{gullysantiago2015,faria2016,keshavan2017,leonard2017,jordan2017,peterson2017,hahn2017,pricewhelan2017}. Specific examples follow below.

\subsection*{Examples of Hack Week Outcomes}
\label{sec:outcomes}
\subsubsection*{Example 1: Astro Hack Week}
In 2015, a small team used AHW to found a new software project called Stingray\footnote{https://github.com/StingraySoftware/stingray} with the goal of providing implementations of time series analysis algorithms often used in astronomy.
Astro Hack Week enabled participants to seed a new collaboration around a software project needed by the larger community, facilitated by the collaborative environment at AHW.
Stingray has since matured into an enduring collaboration within the community with five active maintainers and four Google Summer of Code projects.
\subsubsection*{Example 2: Geo Hack Week}
In 2016, a GHW project team used Google Earth Engine to explore spatial patterns in climate, topography and population data with the goal of mapping the most suitable locations for renewable energy sites in the United States.
The team used machine learning algorithms in conjunction with the powerful hardware resources provided by Google Earth Engine\footnote{\url{http://georgerichardson.net/2017/04/10/searching-for-energy-in-a-random-forest/}}.
%George Richardson, one of the project leads, now works for a renewable resource company in Seattle.
\subsubsection*{Example 3: Neuro Hack Week}
During NHW 2016, one of the teams analyzed an openly available data-set of MRI data from children\cite{Di_Martino2014-qf}, to test the effects of motion on analysis results, using varying motion cut-offs.
%They tested both the split-half reliability of an analysis of brain connectivity, as well as an analysis that used machine learning to distinguish between brains of children with and without autism spectrum disorder.
The team (all from  different institutions) continued to work on this project remotely after the end of NHW, eventually publishing a paper describing these results \cite{leonard2017}.
