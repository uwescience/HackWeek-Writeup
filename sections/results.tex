\section*{Results}

\begin{figure*}[h!]
%\begin{center}
%\begin{subfigure}[t]{\textwidth}
\centering
%\caption{}
\includegraphics[width=\textwidth]{fig/f2.eps}
%\end{subfigure}

\caption{{\bf Post-workshop surveys from three hack weeks}: participants in the 2016 astro-, geo- and neuro- hack weeks responded to questions assessing their experiences. For GHW and NHW, response rates were 100\%, with $N_{\mathrm{GHW}}= 83$ and $N_{\mathrm{NHW}} = 86$, respectively. At AHW, the combined response rate for both years was 76\%, or $N_{\mathrm{AHW}} = 72$ out of $94$ total attendees. We report here about results in three different domains: the development of technical skills (a -- e), collaboration and learning (f,g), and shifts in attitudes towards reproducibility and open science (h -- j)}
\label{fig:survey}
%\end{center}
\end{figure*}

Measuring the success of a hack week objectively is complicated by the variety of objectives that a hack week might have (see above).
Additionally, the participant-driven, open-ended format facilitates knowledge transfer and collaborations in sometimes surprising ways that escape traditional measures of success.

One key metric is the number of publications that result from hack week projects, but this is a fairly narrow definition of success, in line with standard academic performance indicators.
Assuming that participants work largely in the open during a hack week, and that most projects have a strong programming component another indicator of success is the activity of participants in terms of code written and committed to a public code repository.
Still, these measures ignore learning, community-building as well as networking outcomes, which can be assessed through post-workshop surveys.
%It is in principle possible to measure gains in knowledge, networks and productivity within the pool of acceptable candidates for both those that attended the hack week and those that did not.
%This would then provide a somewhat objective measure of the impact the hack week has had.
%In practice, this has not been done for any of the hack weeks conducted thus far.
Here, we have taken an approach that combines these metrics: we start with survey results, and anecdotally report about publications, new code and projects generated (see the following section).
%Open-ended questions allow participants to provide feedback about outcomes, problems and goals that organizers had not anticipated.

%If hack week organizers plan to conduct research that involves hack week participants (for example, using assessments and evaluations in a paper such as this one) it is important to obtain approval from the Institutional Review Board (IRB), or equivalent body that approves research with human participants at the institution hosting the hack week.
%Though this research would usually fall under the category of ''minimal risk``, it is still important to establish procedures to manage these data, and to obtain informed consent.
%In particular, in some cases hack week organizers may be interested in studying not only the participants in the hack week, but also the applicants who did not end up participating.
%It is important to establish procedures to conduct such studies and to obtain approval from an IRB.

Focusing on the outcomes of astro-, geo- and neuro- hack weeks (AHW, GHW, NHW, respectively) from 2016 and 2017, we find that most participants self-report successful learning outcomes (AHW: 76\%, GHW: 89\%, NHW: 79\% for responses ``somewhat agree'', ``agree'' and ``strongly agree''; Figure \ref{fig:survey}, (a)).
The overwhelming majority of respondents at the hack weeks ($>95\%$ for all three events) believed that they learned things that improve their day-to-day research, and that attendance has made them a better scientist (Figure \ref{fig:survey}, (b, c) ).
More specifically, we compared learning outcomes in data visualization (the only topic explicitly shared between all three hack weeks; Figure \ref{fig:survey}, (d, e). We asked participants to subjectively rate their knowledge before the hack week, and find skill levels to be broadly distributed. This is expected, given the goal of diversity during the selection stage. We find that most participants have positive learning outcomes at all three hack weeks for data visualization, but that outcomes vary strongly, as is expected, too, with a group that is this diverse. We also note that there might be systematic effects and biases due to the subjectivity of rating knowledge and learning outcomes.

The majority of participants felt that they built valuable connections to other researchers (Figure \ref{fig:survey}, (g)), especially at Neuro Hack Week, where more than 64\% of participants strongly agreed with this statement.
Because peer learning is a major mode of knowledge transfer at hack weeks, we asked participants whether they taught other participants.
We find that again a majority agrees with this statement to some degree (AHW: 79\%, GHW: 69\%, NHW: 75\%; Figure \ref{fig:survey}, (f)), though responses are not as unequivocal as they are in some of the other categories.
Similarly to learning outcomes, this is expected given the range of skill levels at the workshops. It is notable, however, that we find no correlation between the response to this question and career stage for any of the events surveyed here. Participants at all career stages report similar engagement in teaching. This suggests there is some evidence for our hypothesis that compared to e.g.\ a traditional summer school, a hack week is less hierarchical and encourages lateral knowledge transfer between participants at different career stages.  We similarly find no correlations with gender identity or race/ethnicity.

We find that the hack weeks have been largely successful at efforts to promote positive attitudes towards reproducibility and open science: at all three events, more than 85\% of all participants (AHW: 86\%, GHW: 94\%, NHW: 95\%; Figure \ref{fig:survey}, (h)) put code or data created at the hack week into a public repository, while a substantially smaller fraction of participants followed a regular practice of publishing code before the event (Figure \ref{fig:survey}, (i)).
The overall behaviour reflects how general conventions and attitudes differ in different fields.
Astronomy shows the largest degree of openness toward open science, whereas our results indicate that open science is still fairly uncommon in the geosciences, with neuroscience falling in between.
%This implies that hack weeks can have the highest impacts in field where a priori engagement in reproducibility efforts is low and significant progress can be made towards changing researchers' attitudes during a collaborative workshop.
Similar attitudes are reflected when asking whether the hack week has made participants more comfortable with open science (Figure \ref{fig:survey}, (j)): again, geoscience shows the large improvement with over 97\% agreeing with this statement to some degree, followed by neuroscience (95\%) and astronomy (72\%).
Overall, our results indicate that hack weeks are effective at addressing persisting doubts about making research open and reproducible. While the focus on open science is not necessarily a required component of a hack week, it aligns naturally with many of the goals and values commonly promoted at hack weeks like open-source software and data sharing. In some fields, especially where ethical issues around data sharing and privacy are relevant, this might not be a desired focus of the hack week, or might be replaced or augmented by a discussion of ethical considerations.

%From the very nature of the activities that we encourage in hack weeks, participants in these events produce digital records of their research online. This means that it will be fairly straightforward to evaluate the long-term impact of these activities on participants' productivity (e.g., through contributions to open-source software) in the future.
Because all three events are relatively recent, it is still early to evaluate long-term outcomes, as well as others including publications and collaborations resulting from these events.
There are, however, initial indicators that all hack weeks encouraged long-term engagement with new concepts or tools and that they directly resulted in a number of publications \cite{gullysantiago2015,faria2016,keshavan2017,leonard2017,jordan2017,peterson2017,hahn2017,pricewhelan2017}. For specific examples, see also below.

\subsection*{Examples of Hack Week Outcomes}
\label{sec:outcomes}
\subsubsection*{Example 1: Astro Hack Week}
In 2015, a small team used AHW to found a new software project called Stingray\footnote{https://github.com/StingraySoftware/stingray} with the goal of providing well-tested implementations of time series analysis algorithms often used in X-ray astronomy.
The start of this project was facilitated by the collaborative environment at Astro Hack Week, including expertise in how to start/run open-source projects and role models of successful projects. Astro Hack Week enabled participants to seed a new collaboration around a software project needed by the larger community.
Stingray has since matured into an enduring collaboration within the community with five active maintainers and four Google Summer of Code projects.
\subsubsection*{Example 2: Geo Hack Week}
In 2016, a GHW project team used Google Earth Engine to explore spatial patterns in climate, topography and population data with the goal of mapping the most suitable locations for renewable energy sites in the United States.
The team used machine learning algorithms in conjunction with the powerful hardware resources provided by Google Earth Engine\footnote{\url{http://georgerichardson.net/2017/04/10/searching-for-energy-in-a-random-forest/}}.
George Richardson, one of the project leads, now works for a renewable resource company in Seattle.
\subsubsection*{Example 3: Neuro Hack Week}
Motion of study participants inside of the MRI machine is a major concern in neuroimaging studies, particularly in studies of children or patients, as they are more likely to move.
During NHW 2016 one of the teams focused on a large and openly available data-set of MRI data from children\footnote{ABIDE: \url{http://preprocessed-connectomes-project.org/abide}}.
To test the effect of motion on the results, the team conducted an analysis in which both the number of experimental subjects included, as well as motion cut-off were varied.
%They tested both the split-half reliability of an analysis of brain connectivity, as well as an analysis that used machine learning to distinguish between brains of children with and without autism spectrum disorder.
The team (composed of four different researchers from four institutions) continued to work on this project remotely after the end of NHW, and eventually published a paper describing these results in the open access journal Research Ideas and Outcomes \cite{leonard2017}.
