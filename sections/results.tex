\section*{Results}

\begin{figure*}[h!]
%\begin{center}
\begin{subfigure}[t]{\textwidth}
\centering
\caption{}
\includegraphics[width=11cm]{fig/eval_techskills.eps}
\end{subfigure}

\begin{subfigure}[t]{\textwidth}
\centering
\caption{}
\includegraphics[width=11cm]{fig/longitudinal_visualization.eps}
\end{subfigure}

\begin{subfigure}[t]{\textwidth}
\centering
\caption{}
\includegraphics[width=11cm]{fig/eval_collab.eps}
\end{subfigure}

\begin{subfigure}[t]{\textwidth}
\centering
\caption{}
\includegraphics[width=11cm]{fig/eval_openscience.eps}
\end{subfigure}

\caption{{\bf Post-workshop surveys from three hack weeks}: participants in the 2016 astro-, geo- and neuro- hack weeks responded to questions assessing their experiences. \textbf{For GHW and NHW, response rates were 100\%, with $N_{\mathrm{GHW}}= 83$ and $N_{\mathrm{NHW}} = 86$, respectively. At AHW, the combined response rate for both years was 76\%, or $N_{\mathrm{AHW}} = 72$ out of $94$ total attendees. We report here about results in three different domains: the development of technical skills (a and b), collaboration and learning (c), and shifts in attitudes towards reproducibility and open science (d)}}
\label{fig:survey}
%\end{center}
\end{figure*}

Measuring the success of a hack week objectively is complicated by the variety of objectives that a hack week might have (see above). 
Additionally, the participant-driven, open-ended format facilitates knowledge transfer and collaborations in sometimes surprising ways that escape traditional measures of success.

One key metric is the number of publications that result from hack week projects, but this is a fairly narrow definition of success, in line with standard academic performance indicators.
Assuming that participants work largely in the open during a hack week, and that most projects have a strong programming component another indicator of success is the activity of participants in terms of code written and committed to a public code repository.
Still, these measures ignore learning, community-building as well as networking outcomes, which can be assessed through post-workshop surveys.
%It is in principle possible to measure gains in knowledge, networks and productivity within the pool of acceptable candidates for both those that attended the hack week and those that did not.
%This would then provide a somewhat objective measure of the impact the hack week has had.
%In practice, this has not been done for any of the hack weeks conducted thus far.
Here, we have taken an approach that combines these metrics: we start with survey results, and anecdotally report about publications, new code and projects generated (see the following section).
%Open-ended questions allow participants to provide feedback about outcomes, problems and goals that organizers had not anticipated.

%If hack week organizers plan to conduct research that involves hack week participants (for example, using assessments and evaluations in a paper such as this one) it is important to obtain approval from the Institutional Review Board (IRB), or equivalent body that approves research with human participants at the institution hosting the hack week.
%Though this research would usually fall under the category of ''minimal risk``, it is still important to establish procedures to manage these data, and to obtain informed consent.
%In particular, in some cases hack week organizers may be interested in studying not only the participants in the hack week, but also the applicants who did not end up participating.
%It is important to establish procedures to conduct such studies and to obtain approval from an IRB.

Focusing on the outcomes of astro-, geo- and neuro- hack weeks (AHW, GHW, NHW, respectively) from 2016, we find that \textbf{most} participants self-report successful learning outcomes.  in new topics, tools or methods (AHW: 76\%, GHW: 89\%, NHW: 79\% for responses ``somewhat agree'', ``agree'' and ``strongly agree''; Figure \ref{fig:survey}, (a), left).
The overwhelming majority of respondents at the hack weeks ($>95\%$ for all three events) believed that they learned things that improve their day-to-day research, and that attendance has made them a better scientist (Figure \ref{fig:survey}, (a), right and middle).
\textbf{More specifically, we compared learning outcomes in data visualization (the only topic explicitly shared between all three hack weeks; Figure \ref{fig:survey}, (b), left). We asked participants to subjectively rate their knowledge before the hack week, and find skill levels to be broadly distributed. This is expected, given the goal of diversity during the selection stage, but we also note that there might be systematic effects, since the way the question was posed may be subject to individual biases. We then asked participants to rate their learning outcomes at the hack weeks in this category, and find that most participants have positive learning outcomes at all three hack weeks for data visualization, but that outcomes vary strongly, as is expected, too, with a group that is this diverse \textit{a priori}.}
%While each hack week also probed participants' attitudes in more detail toward specific topics, methods, and modes of learning, the hack weeks differed substantially on that level, making the results difficult to compare.
%Another important goal of the hack weeks was centred on community building and fostering collaborations.
The majority of participants felt that their contributions to their hack teams was valued, and that they built valuable connections to other researchers (Figure \ref{fig:survey}, middle panel, middle).
This is especially true for Neuro Hack Week, where more than 64\% of participants strongly agreed that they formed valuable connections at NHW (Figure  \ref{fig:survey}, middle panel, right).
Because peer learning is a major mode of knowledge transfer at hack weeks, we asked participants whether they taught other participants.
We find that again a majority agrees with this statement to some degree (AHW: 79\%, GHW: 69\%, NHW: 75\% for combined responses ``somewhat agree'', ``agree'' and ``strongly agree''; Figure \ref{fig:survey}, middle panel, left), though responses are not as unequivocal as they are in some of the other categories.
This is expected: participants new to the field may participate to learn rather than to teach. \textbf{It is notable, however, that we find no correlation between the response to this question and career stage for any of the events surveyed here. In particular, participants from all career stages between graduate students and tenured faculty report similar engagement in teaching. This supports our hypothesis that compared to e.g.\ a traditional summer school, a hack week is less hierarchical and encourages lateral knowledge transfer between participants at different career stages.  We similar find no correlations with gender identity or race/ethnicity. Responses to the other two questions regarding whether participants' contributions to hack teams were valued and whether they made valuable connections at the hack week were similarly evenly distributed across career stages, gender and racial/ethnic identities.}

We find that the hack weeks have been largely successful at efforts to promote positive attitudes towards reproducibility and open science: at all three events, more than 85\% of all participants (AHW: 86\%, GHW: 94\%, NHW: 95\%; Figure \ref{fig:survey}, bottom panel, left) put code or data created at the hack week into a public repository, while a substantially smaller fraction of participants followed this practice before the event (Figure \ref{fig:survey}, bottom panel, middle).
When asking participants whether they had made their code or data openly available in the past, the overall behaviour reflects how general conventions and attitudes differ in different fields.
Astronomy shows the largest degree of openness toward open science, whereas our results indicate that open science is still fairly uncommon in the geosciences, with neuroscience falling in between.
%This implies that hack weeks can have the highest impacts in field where a priori engagement in reproducibility efforts is low and significant progress can be made towards changing researchers' attitudes during a collaborative workshop.
Similar attitudes are reflected when asking whether the hack week has made participants more comfortable with open science: again, geoscience shows the large improvement with over 97\% agreeing with this statement to some degree, closely followed by neuroscience (95\%), while there was somewhat less of an impact on participants' attitudes in astronomy (72\%).
Overall, our results show that hack weeks are effective at addressing persisting doubts about making research open and reproducible. \textbf{While the focus on open science is not necessarily a required component of a hack week, it aligns naturally with many of the goals and values commonly promoted at hack weeks, including open-source software, data sharing and reproducibility. In some fields, especially where ethical issues around data sharing and privacy are relevant, this might not be a desired focus of the hack week, or might be replaced or augmented by a discussion of ethical considerations.}

From the very nature of the activities that we encourage in hack weeks, participants in these events produce digital records of their research online. This means that it will be fairly straightforward to evaluate the long-term impact of these activities on participants' productivity (e.g., through contributions to open-source software) in the future. Because all three events are relatively recent, it is still early to evaluate such long-term outcomes, as well as others including publications and collaborations resulting from these events.
There are, however, initial indicators that all hack weeks encouraged long-term engagement with new concepts or tools and that they directly resulted in a number of publications \cite{gullysantiago2015,faria2016,keshavan2017,leonard2017,jordan2017,peterson2017,hahn2017,pricewhelan2017}. For specific examples, see also below.

\subsection*{Examples of Hack Week Outcomes}
\label{sec:outcomes}
\subsubsection*{Example 1: Astro Hack Week}
In 2015, a small team used the opportunity of AHW to found a new software project called Stingray\footnote{https://github.com/StingraySoftware/stingray} with the goal of providing well-tested, well-documented implementations of time series analysis algorithms often used in X-ray astronomy.
The start of this project was facilitated by the collaborative environment at Astro Hack Week, including expertise in how to start/run open-source projects, role models of successful projects, and an environment encouraging scientific risk taking. Astro Hack Week enabled participants to seed a new collaboration around a software project needed by the larger community.
Since its beginnings at Astro Hack Week, Stingray has matured into an enduring collaboration within the community with five active maintainers, a number of contributors and four Google Summer of Code projects.
\subsubsection*{Example 2: Geo Hack Week}
In 2016, a GHW project team used Google Earth Engine to explore spatial patterns in climate, topography and population data with the goal of mapping the most suitable locations for renewable energy sites in the United States.
The team used machine learning algorithms in conjunction with the powerful hardware resources provided by Google Earth Engine\footnote{\url{http://georgerichardson.net/2017/04/10/searching-for-energy-in-a-random-forest/}}.
George Richardson, one of the project leads, now works for a renewable resource company in Seattle.
\subsubsection*{Example 3: Neuro Hack Week}
Motion of study participants inside of the MRI machine is a major concern in neuroimaging studies, particularly in studies of children or patients, as they are more likely to move.
During NHW 2016 one of the teams focused on a large and openly available data-set of MRI data from children\footnote{ABIDE: \url{http://preprocessed-connectomes-project.org/abide}}.
To test the effect of motion on the results, the team conducted an analysis in which both the number of experimental subjects included, as well as motion cut-off were varied.
%They tested both the split-half reliability of an analysis of brain connectivity, as well as an analysis that used machine learning to distinguish between brains of children with and without autism spectrum disorder.
The team (composed of four different researchers from four different institutions in two different countries) continued to work on this project remotely after the end of Neuro Hack Week, and eventually published a paper describing these results in the open access journal Research Ideas and Outcomes \cite{leonard2017}.
